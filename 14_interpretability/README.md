# Interpretability


## Overview
Attribution, probing, patching, and circuit-level analysis for Transformers.

## Key Papers
- Olah et al., 2020â€“2021: Transformer Circuits
- Geva et al., 2021: MLP as key-value memories
- Meng et al., 2022: ROME editing

## Tutorials
- Captum/TransformerLens intros

## Code Starters
- `src/attention_viz.py`
- `src/activation_patching.py`

## Exercises
- [ ] Visualize head patterns and run causal tracing
- **Deliverables**: narrative report + safety observations
- **Success Metrics**: Replicate at least one known head behavior
