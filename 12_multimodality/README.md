# Multimodality


## Overview
Vision-language pretraining, contrastive learning, and instruction tuning.

## Key Papers
- Radford et al., 2021: CLIP
- Alayrac et al., 2022: Flamingo
- (Survey) Vision-Language Models

## Tutorials
- TorchVision + tokenizers integration

## Code Starters
- `src/clip_mini.py`
- `src/mm_dataset_loader.py`

## Exercises
- [ ] Train a mini-CLIP on a small dataset
- **Deliverables**: retrieval R@K, qualitative gallery
- **Success Metrics**: R@1 > baseline linear probe
