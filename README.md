# LLM Research Playbook

A ready-to-clone, end-to-end blueprint for becoming an **LLM researcher** and building publishable projects.
Created: 2025-08-29

## What's Inside
- **15 topic modules**: each with a roadmap, references, code starters, and exercises.
- **Docs**: concise paper notes, cheat-sheets.
- **Notebooks**: quick experiments and visualizations.
- **Utils**: reusable dataset loaders, training loops, metrics, plotting.
- **Research Projects**: capstone experiments (paper-ready).
- **Starter Skeletons**: boilerplate for new experiments.

## How to Use
1. Create a new private/public repo and push this folder.
2. Work through modules 01→15 in order (or pick-and-choose).
3. Track progress using *Deliverables* and *Success Metrics* listed per module.
4. Use `research_projects/` for anything intended for arXiv/conferences.

## Global Deliverables & Success Metrics
- **GitHub Repo Released** (public or private)
- **Reproducible Training Script** (`python train.py --config ...`)
- **Experiment Report** (Markdown or Jupyter)
- **arXiv Preprint** (optional LaTeX template in `docs/` you can add)
- **Conference Submission** (NeurIPS/ICLR/ACL/EMNLP)
- **Benchmark Table** (baselines vs. your approach)
- **Ablation Study** (at least 3 dimensions)

## Suggested Weekly Timeline (24 weeks, ~6 months)
- Weeks 1–2: 01 Foundations
- Weeks 3–4: 02 Embeddings
- Weeks 5–6: 03 Seq2Seq RNN + 04 Attention
- Weeks 7–8: 05 Transformer From Scratch + 06 Annotated Transformer
- Weeks 9–10: 07 Pretraining & Scaling
- Weeks 11–12: 08 Fine-tuning & PEFT
- Weeks 13–14: 09 RAG & Retrieval
- Weeks 15–16: 10 RLHF & Alignment
- Weeks 17–18: 11 Reasoning & CoT
- Weeks 19–20: 12 Multimodality
- Weeks 21–22: 13 Long-Context & Memory
- Weeks 23: 14 Interpretability
- Week 24: 15 Deployment & Serving + convert a capstone into a **paper submission**

> Tip: Treat each module README as a **checklist**. Use the boxes ✓ to track completion.
