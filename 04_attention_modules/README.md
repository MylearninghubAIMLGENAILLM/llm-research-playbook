# Attention Modules


## Overview
Additive (Bahdanau), multiplicative (Luong), scaled dot-product attention and multi-head attention.

## Key Papers
- Bahdanau et al., 2014
- Luong et al., 2015
- Vaswani et al., 2017: Scaled dot-product attention

## Tutorials
- Jay Alammar: The Illustrated Transformer
- CS224n attention lecture

## Code Starters
- `src/scaled_dot_product_attention.py`
- `src/multi_head_attention.py`

## Exercises
- [ ] Implement scaled dot-product attention + masking
- [ ] Visualize attention maps
- **Deliverables**: correctness tests + visualizations
- **Success Metrics**: Unit tests pass; visualization sanity checks
